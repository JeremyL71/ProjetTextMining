{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # MOTEUR DE RECHERCHE\n",
    " par **Alexandre DUFOUR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ce document s'inscrit dans le périmètre du projet tutoré de l'un des groupes de la classe d'Année SPEciale de l'IUT Informatique LYON 1.\n",
    "Réunissant :\n",
    "- DUFOUR Alexandre\n",
    "- FERRAND Gérome\n",
    "- LAURENT Jeremy\n",
    "- MOUSSU Nathan\n",
    "- VATON Juliette\n",
    "\n",
    "Ce groupe était placé sous le tutorat de M. VIDAL VINCENT.\n",
    "\n",
    "Ce document représente les travaux de **Alexandre DUFOUR**, au sujet d'un programme de **moteur de recherche**.\n",
    "\n",
    "A terme, le code inscrit dans ce document sera ré-employé dans un programme plus conséquent, réunissant les applications suivantes :\n",
    "- Moteur de recherche (affecté à DUFOUR Alexandre)\n",
    "- Détecteur de spams (affecté à LAURENT Jérémy)\n",
    "- Analyseur d'opinion (affecté à VATON Juliette)\n",
    "\n",
    "## Notre OBJECTIF :\n",
    "\n",
    "Le but de cette application est de permettre à un utilisateur de rechercher un texte dans un corpus à partir d'une requête tapée au clavier. Comme pour un moteur de recherche web, la requête est préférablement une suite de mots-clés en rapport avec le texte recherché.\n",
    "\n",
    "\n",
    "## A propos du CORPUS :\n",
    "\n",
    "L'application n'est pas réalisée pour un corpus particulier. Le corpus utilisé ici sera le jeu de données 20newgroups, simplement car il est déjà utilisé par l'application de catégorisation de texte, ce qui permet de pouvoir partager les connaissances.<br>\n",
    "20newsgroups est une collection de près de 20 000 documents répartis sur 20 thèmes (recueilli par Ken Lang). Cette collection est devenue très populaire parmi les informaticiens désireux de faire des expériences dans le domaine du text mining.<br>\n",
    "(Nous trouverons plus d'infos sur le dataset ici : http://qwone.com/~jason/20Newsgroups/)\n",
    "\n",
    "\n",
    "## Vers de la DOCUMENTATION COMPLEMENTAIRE :\n",
    "\n",
    "Cette présentation a été conçue grâce aux documentations suivantes :\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html<br>\n",
    "http://www.nltk.org/\n",
    "\n",
    "## Plan du document :\n",
    "\n",
    "1. **Principe de l'algorithme**\n",
    "    1. **Traitement du corpus**\n",
    "        1. **Transformation des textes**\n",
    "        1. **Scoring**\n",
    "    1. **Traitement d'une recherche**\n",
    "        1. **Vectorisation de la requête**\n",
    "        1. **Calcul de similarité**\n",
    "1. **Exécution de l'application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principe de l'algorithme\n",
    "\n",
    "L'algorithme se base sur l'analyse de la fréquence des mots dans les textes, et fonctionne en deux phases distinctes :\n",
    "\n",
    "- Une première phase de travail préliminaire de traitement du corpus, qui a pour but de construire une matrice représentatrice de ce dernier\n",
    "- La phase de recherche à proprement parler, qui consiste représenter de manière vectorielle la requête de l'utilisateur et trouver la ligne de la matrice du corpus (donc le texte) lui correspondant le plus\n",
    "\n",
    "### 1.1. Traitement du corpus\n",
    "\n",
    "Le but est à partir du corpus constitué de textes sous la forme d'une chaîne de caractère de construire une matrice représentatrice de ce dernier. Chaque ligne de la matrice représentera un texte et chaque colonne représentera un mot présent dans le corpus. **La valeur dans la matrice à la place (i, j) correspond à l'importance du mot j pour le texte i par rapport au reste du corpus.** Cette valeur doit permettre de choisir parmis les textes, ainsi l'importance du mot doit être comprise comme importance au sein du texte en lui-même (répétition du mot dans le texte) et importance dans ce texte par rapport aux autres textes (mot présent dans le texte mais peu dans le reste du corpus).\n",
    "\n",
    "#### 1.1.1 Transformation des textes\n",
    "\n",
    "Pour construire la matrice, on commence par \"découper\" les textes en listes de mots (**tokenisation**). On va ensuite éliminer les mots trop communs (**stopwords**), que l'on considère comme ayant trop peu de sens pour être pris en compte (cela peut aussi être la ponctuation). On va ensuite appliquer aux mots des transformations, afin de rassembler ceux ayant un sens identique ou proche, de deux types :\n",
    "- **lemmatisation** : Rassembler des mots sous un même mot-clé\n",
    "- **stemming** (ou radicalisation) : Rassembler des mots sous un même radical\n",
    "\n",
    "On utilise pour cela la bibliothèque **nltk** (Natural Language ToolKit) de python. On définit ainsi la fonction \"split\" suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Import des fonctions et outils utilisé\n",
    "from nltk.corpus import stopwords          # liste des mots non pris en compte\n",
    "from nltk.tokenize import word_tokenize    # fonction de tokenisation\n",
    "from nltk.stem import WordNetLemmatizer    # fonction de lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer # fonction de stemming\n",
    "\n",
    "# \"Hyperparamètres\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# On peut modeler la liste des mots non pris en compte\n",
    "stop_words.update(stop_words,{'.',',','!','?','\\'s', '<', '>', ':', ';', '/', '(', ')', '-', '_', '{', '}', '--', '...'})\n",
    "lmtzr=WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(text):\n",
    "    \"\"\" Fonction qui prend en paramètre un texte (chaine de caractère) et qui \n",
    "    renvoie la liste de ses mots ayant été filtrés puis ayant subits certaines \n",
    "    transformations: lemming puis stemming \"\"\"\n",
    "    \n",
    "    # Tokenisation ************************************************************\n",
    "    # Découpage du texte en mots (words est une liste de chaine de caractère)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Stop-words **************************************************************\n",
    "    # Filtrage des mots : on supprime de words ceux qui sont contenus dans \n",
    "    # stop_words car ils sont supposés avoir trop peu de sens.\n",
    "    # (mots trop communs, mots de liaisons, ponctuation)\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            words_clean.append(word)\n",
    "    \n",
    "    # Lemming *****************************************************************\n",
    "    # Transformation des mots en un unique mot-clé (lemme) les représentant. \n",
    "    # Ex: divided, dividing, divided, divides -> divide\n",
    "    words_lemmed = [lmtzr.lemmatize(word) for word in words_clean]\n",
    "            \n",
    "    # Stemming ****************************************************************\n",
    "    # Transformation des mots en un unique radical les représentant. \n",
    "    # Ex: divided, dividing, divided, divides -> divid\n",
    "    words_stemmed = [ps.stem(word) for word in words_lemmed]\n",
    "    \n",
    "    return words_stemmed\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors identifier les **\"tokens\"** (mots distincts) du corpus, qui forment donc une base de représentation des textes sous forme vectorielle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Scoring\n",
    "\n",
    "Il s'agit maintenant d'affecter un score pour chaque token dans chaque texte, ce qui formera la matrice  désirée. Ce score, comme dit précédemment, est basé sur la fréquence du token, et doit être représentatif à la fois de l'importance du token dans le texte en lui-même et par rapport aux autres textes du corpus.\n",
    "\n",
    "On commence par récupérer les fréquences d'apparition des tokens grâce à la fonction \"count\" suivante :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(words, wordbase):\n",
    "    \"\"\" Fonction qui prend en paramètre un texte 'splité' en mots et la liste \n",
    "    des tokens du corpus, et renvoie le vecteur contenant le nombre d'occurence \n",
    "    dans le texte des tokens du corpus.\"\"\"\n",
    "    \n",
    "    vector = [0 for i in range(len(wordbase))]\n",
    "    for i in range(len(wordbase)):\n",
    "        if wordbase[i] in words:\n",
    "            vector[i] += 1\n",
    "    return vector\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors récupérer la matrice représentant la fréquence de chaque token dans chaque texte. On va alors à partir de ces fréquences calculer un score pour chaque token dans chaque texte selon la formule de **TF-IDF** (Term Frequency - Inverse Document Frequency).\n",
    "\n",
    "La partie TF indique l'importance du token dans le texte. On choisit ici de prendre **TF(t, m) = log ( 1 + F(t, m) )**, où F(t, m) est la fréquence du token m dans le texte t.\n",
    "\n",
    "La partie IDF indique le degré de spécification d'un token, c'est-à-dire que ce critère est d'autant plus important que le token apparaît dans peu de texte. On peut alors dire qu'il est spécifique aux textes dans lesquels il apparraît. On choisit ici de prendre **IDF(m) = log( N / nt(m) )**, où N est le nombre de textes du corpus et nt(m) est le nombre de textes dans lesquels apparaît le token m.\n",
    "\n",
    "Le score d'un token m pour un texte t dans la matrice (qu'on appelle M) est donné par **M(t, m) = TF(t, m) * IDF(m)**\n",
    "\n",
    "On peut maintenant définir la fonction \"preliminaryWork\" qui à partir d'un corpus sous la forme d'une liste de chaîne de caractère, chaque chaîne étant un texte, va construire la matrice des scores des tokens dans les textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preliminaryWork(corpus):\n",
    "    \"\"\" Fonction qui prend en paramètre un corpus de texte sous la forme d'une \n",
    "    liste de chaines de caractère, et qui renvoie la liste des mots utilisés \n",
    "    comme base pour représenter les textes qui le compose, et la matrice de ces\n",
    "    textes dans cette base, les coordonnées étant calculées par la formule du \n",
    "    TF-IDF.\"\"\"\n",
    "    \n",
    "    # Découpage, tri et transformation des textes (voir split)\n",
    "    corpus_words = []\n",
    "    for i in range(len(corpus)):\n",
    "        corpus_words.append(split(corpus[i]))\n",
    "        \n",
    "    # Construction de la liste des mots du corpus (intersection des mots des \n",
    "    # textes). Wordset est un objet de type set, intéressant car il permet de \n",
    "    # faire l'intersection seul, mais pas ordonné. On construit donc wordbase\n",
    "    # à partir des mots de wordset pour pouvoir associer 1 mot à 1 coordonnée.\n",
    "    wordset = set()\n",
    "    for words in corpus_words:\n",
    "        wordset = wordset.union(set(words))\n",
    "    wordbase = [word for word in wordset]\n",
    "    \n",
    "    # Construction de la matrice représentant les textes dans la base wordbase.\n",
    "    # On l'initialise avec les vecteurs dont les coordonnées sont les \n",
    "    # occurences brutes.\n",
    "    matrix = []\n",
    "    for words in corpus_words:\n",
    "        matrix.append(count(words, wordbase))\n",
    "        \n",
    "    # Calcul du nombe de textes contenant chaque mot\n",
    "    nt = [0 for m in range(len(wordbase))]\n",
    "    for m in range(len(wordbase)):\n",
    "        for line in matrix:\n",
    "            if line[m] > 0:\n",
    "                nt[m] += 1\n",
    "    \n",
    "    # Calcul du tf-idf de chaque mot dans chaque texte\n",
    "    # TF(mot dans un texte) = log(1 + nb d'occurence de ce mot dans ce texte)\n",
    "    # IDF(mot) = log(nombre de textes total / nombre de texte cntenant ce mot)\n",
    "    # TF-IDF(mot dans un texte) = TF(mot dans un texte) * IDF(mot)\n",
    "    for t in range(len(matrix)):\n",
    "        for m in range(len(wordbase)):\n",
    "            matrix[t][m] = math.log(1 + matrix[t][m]) * math.log(len(corpus) / nt[m])\n",
    "            \n",
    "    return (matrix, wordbase)\n",
    "#End\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On renvoie aussi la liste des tokens (wordbase) pour les traitements à suivre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Traitement d'une recherche\n",
    "\n",
    "Une fois que la matrice représentatrice du corpus a été construite, on va vouloir traiter les requêtes de l'utilisateur. Pour cela, on va procéder en deux temps :\n",
    "- vectoriser la requête, comme on a vectorisé les textes du corpus\n",
    "- identifier le texte le plus proche de la requête en calculant la similarité entre la requête et chaque texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Vectorisation de la requête\n",
    "\n",
    "On cherche à mettre la requête sous la forme d'un vecteur. Afin que les vecteurs de la requête et des textes soient comparables et que les calculs aient un sens, il faut **appliquer à la requête le même traitement qu'on a appliqué au textes**. Ainsi, on s'assure que les coordonnées de la requêtes portent bien sur les bons tokens.\n",
    "\n",
    "Pour les coordonnées du vecteur, on va simplement associer à chaque token sa fréquence dans la requête.\n",
    "\n",
    "Pour cela, on réutilise les fonctions \"split\" et \"count\" utilisées pour le traitement du corpus, afin de définir la fonction \"vectorisation\" suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisation(text, wordbase):\n",
    "    \"\"\" Fonction qui prend en paramètre un texte sous la forme d'une chaine de \n",
    "    caractère, et la liste des mots du corpus et qui renvoie le vecteur \n",
    "    représentant le texte dans la base du corpus.\"\"\"\n",
    "    \n",
    "    return count(split(text), wordbase)\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Calcul de similarité\n",
    "\n",
    "On va calculer pour chaque texte un score correspondant à sa proximité à la requête. Pour cela on calcule pour chaque vecteur de texte du corpus (donc ligne de la matrice) sa similarité avec le vecteur de la requête.\n",
    "\n",
    "On utilise ici la similarité cosinus qui se calcule de la manière suivante :<br>\n",
    "Sim(t) = <R, T> / ( ||R|| * ||T|| )<br>\n",
    "Où T est le vecteur du texte t (= la ligne M[t]), R le vecteur de la requête,  < , > un produit scalaire et || || la norme associée.\n",
    "\n",
    "On va donc calculer la liste de ces valeurs pour chaque test, puis sélectionner les meilleurs résultats (le nombre de résultat choisi étant arbitraire). On définit donc les fonctions suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Nombre de textes conservés pour une requête\n",
    "nbTop = 10;\n",
    "\n",
    "def scal(v1, v2):\n",
    "    \"\"\" Fonction qui calcule le produit scalaire entre deux vecteurs de même\n",
    "    taille et la renvoie.\"\"\"\n",
    "    \n",
    "    scal = 0\n",
    "    for i in range(len(v1)):\n",
    "        scal += v1[i] * v2[i]\n",
    "    return (scal)\n",
    "#End\n",
    "  \n",
    "    \n",
    "def norm(v):\n",
    "    \"\"\" Fonction qui calcule la norme 2 d'un vecteur et la renvoie.\"\"\"\n",
    "    n = math.sqrt(scal(v, v))\n",
    "    if (n == 0):\n",
    "        n = 1\n",
    "    return (n)\n",
    "#End\n",
    "\n",
    "\n",
    "def iMax(similarity):\n",
    "    \"\"\" Fonction qui renvoie l'indice du maximum de la liste passée en \n",
    "    paramètre \"\"\"\n",
    "    imax = 0\n",
    "    for i in range(1,len(similarity)):\n",
    "        if (similarity[i] > similarity[imax]):\n",
    "            imax = i\n",
    "    return (imax)\n",
    "#End\n",
    "\n",
    "\n",
    "def top(similarity, nbTop):\n",
    "    \"\"\" Fonction qui renvoie la liste des indices des nbTop éléments \n",
    "    les plus grands de la liste passée en paramètre (similarity), \n",
    "    dans l'ordre décroissant. \"\"\"\n",
    "    order = []\n",
    "    cptTop = 0\n",
    "    imax = iMax(similarity)\n",
    "    while (similarity[imax] >= 0 and cptTop < nbTop):\n",
    "        order.append(imax)\n",
    "        # On met à -1 l'élément dont on vient de prendre l'indice pour ne plus le prendre en compte\n",
    "        similarity[imax] = -1\n",
    "        cptTop += 1\n",
    "        imax = iMax(similarity)\n",
    "    return (order)\n",
    "#End\n",
    "\n",
    "\n",
    "def research(request, matrix, wordbase):\n",
    "    \"\"\" Fonction qui prend en paramètre une requête sous la forme d'une\n",
    "    chaîne de caractère, la matrice représentatrice du corpus et la liste\n",
    "    des tokens, et qui renvoie la liste des indices des textes dans le\n",
    "    corpus correspondant le plus à la requête. \"\"\"\n",
    "    # Vectorisation de la requête\n",
    "    vector = vectorisation(request, wordbase)\n",
    "    # Calcul de la liste des score de similarité pour chaque vecteur de la matrice\n",
    "    similarity = []\n",
    "    for v2 in matrix:\n",
    "        similarity.append(scal(vector, v2) / (norm(vector) * norm(v2)))\n",
    "    # On renvoie les meilleurs résultats\n",
    "    return (top(similarity, nbTop))\n",
    "#End\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exécution de l'application\n",
    "\n",
    "Les fonctions définies précédemment permettent de faire les calculs nécessaires à l'application, et d'avoir des résultats exploitables, il faut alors définir comment se déroule les interractions avec l'utilisateur : récupération de la requête et affichage des résultats. Pour des raisons de simplicité, tout ce fait dans la console python.\n",
    "\n",
    "On définit une fonction d'affichage des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResearch(order, corpus):\n",
    "    \"\"\" Fonction qui à partir des résultats d'une requête (liste d'indices) et \n",
    "    du corpus utilisé affiche les textes dans la console, le plus représentatif\n",
    "    en premier\"\"\"\n",
    "    print(\"*\\t*\\t*\\t*\\t*\\t*\\t*\\t*\")\n",
    "    print(\"*\\t*\\tRésultats de la recherche\\t*\\t*\")\n",
    "    print(\"*\\t*\\t*\\t*\\t*\\t*\\t*\\t*\\n\\n\")\n",
    "    input(\"Appuyer sur une touche...\\n\")\n",
    "    for i in range(len(order)):\n",
    "        print(\"*\\t*\\t*\\t*\\t*\\t*\\t*\\t*\")\n",
    "        print(\"*\\t*\\tNuméro \" + str(i + 1) + \" - Texte \" + str(order[i]) + \"\\t*\\t*\\t*\")\n",
    "        print(\"*\\t*\\t*\\t*\\t*\\t*\\t*\\t*\\n\\n\")\n",
    "        print(corpus.data[order[i]])\n",
    "        # Pause entre l'affichage de chaque texte\n",
    "        input(\"Appuyer sur une touche...\\n\")\n",
    "    print(\"Fin\")\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et enfin la fonction principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus utilisé\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "corpus = fetch_20newsgroups()\n",
    "\n",
    "# Nombre de texte qu'on prend en compte (optionnel)\n",
    "nbTxt = 100\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# main\n",
    "\n",
    "# Construction de la matrice\n",
    "(matrix, wordbase) = preliminaryWork(corpus.data[:nbTxt])\n",
    "\n",
    "request = input(\"Recherche : \")\n",
    "while request != \"exit\":\n",
    "    # Traitemtn d'une recherche\n",
    "    order = research(request, matrix, wordbase)\n",
    "    printResearch(order, corpus)\n",
    "    request = input(\"Recherche : \")\n",
    "\n",
    "#End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
